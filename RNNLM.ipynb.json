{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"RNNLM.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7xoUBOGkGZog","colab_type":"code","outputId":"29fb4ec9-a22b-40dc-f9d4-a3ed344b55f0","executionInfo":{"status":"ok","timestamp":1575586915625,"user_tz":300,"elapsed":372,"user":{"displayName":"Yining Liu","photoUrl":"","userId":"03197070414203380537"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Load packages\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# torch.manual_seed(1)\n","\n","\n","from torch.autograd import Variable\n","import os\n","import numpy as np\n","\n","MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/NLP/Word-Embeddings-and-Language-Modeling/weights/model2.h5\"\n","TRN_RESULT_PATH = '/content/drive/My Drive/Colab Notebooks/NLP/Word-Embeddings-and-Language-Modeling/trn_log_ws.txt'\n","DEV_RESULT_PATH = '/content/drive/My Drive/Colab Notebooks/NLP/Word-Embeddings-and-Language-Modeling/dev_log_ws.txt'\n","EPOCH_SIZE = 30\n","\n","# input_dim = 5\n","# hidden_dim = 10\n","# n_layers = 1\n","\n","# batch_size = 1\n","# seq_len = 3\n","\n","# Google Colab stuff\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O6eJyslWGZom","colab_type":"code","outputId":"dc8efb3b-bebc-49c5-fe86-11635a35d8c9","executionInfo":{"status":"ok","timestamp":1575257427461,"user_tz":300,"elapsed":526,"user":{"displayName":"Yining Liu","photoUrl":"","userId":"03197070414203380537"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gallinae'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"6ze5M8u4GZow","colab_type":"code","colab":{}},"source":["def load_data(path):\n","    # Load data\n","    trn_texts = open(path+\"trn-wiki.txt\").read().strip().split(\"\\n\")\n","    dev_texts = open(path+\"dev-wiki.txt\").read().strip().split(\"\\n\")\n","    \n","\n","    #tokenize data\n","    trn_tokens = []\n","    for trn_text in trn_texts:\n","      tokens = trn_text.split(\" \")\n","      if len(tokens) > 0: trn_tokens.append(tokens)\n","    dev_tokens = []\n","    for dev_text in dev_texts:\n","      tokens = dev_text.split(\" \")\n","      if len(tokens) > 0: dev_tokens.append(tokens)\n","    # trn_tokens = [trn_text.split(\" \") for trn_text in trn_texts]\n","    # dev_tokens = [dev_text.split(\" \") for dev_text in dev_texts]\n","\n","    print(\"Training data ...\")\n","    print(\"%d\" % (len(trn_tokens)))\n","    print(\"Development data ...\")\n","    print(\"%d\" % (len(dev_tokens)))\n","    \n","    #get max feature size\n","    max_length = max(max([len(tokens) for tokens in trn_tokens]), max([len(tokens) for tokens in dev_tokens]))\n","    \n","    return trn_tokens, dev_tokens, max_length\n","\n","\n","def buildVocabulary(trn_tokens):\n","    words = set()\n","    vocab = {}\n","    index = 0\n","    for tokens in trn_tokens:\n","        for token in tokens:\n","            if token not in vocab:\n","                vocab[token] = index\n","                index += 1\n","    return vocab\n","\n","\n","# def index_tokens(tokens, vocabulary):\n","#     tokens_index_list = []\n","#     for tokens in tokens_list:\n","#         tokens_index = [vocabulary[token] for token in tokens]\n","#         tokens_index_list.append(tokens_index)\n","#     return tokens_index_list    \n","\n","def prepare_sequence(tokens, vocabulary):\n","#     tokens_index_list = []\n","#     for tokens in tokens_list:\n","#         tokens_index = [vocabulary[token] for token in tokens]\n","#         tokens_index_list.append(tokens_index)\n","    tokens_index = [vocabulary[token] for token in tokens]\n","    return tokens_index    \n","\n","    \n","class LSTMLanguageModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim = 32, batch_size = 1, hidden_dim=32, num_layers=1):\n","        super(LSTMLanguageModel, self).__init__()\n","        self.batch_size = batch_size\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.embedding_dim = embedding_dim\n","        \n","        # Initialize a default nn.Embedding as word embeddings\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        \n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = num_layers)\n","        \n","        # Linear mid layer to transfer hidden to probablities\n","        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.zeros((1, 1, self.hidden_dim), device=device),\n","                torch.zeros((1, 1, self.hidden_dim), device=device))\n","        \n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        # lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), self.batch_size, -1))\n","        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), self.batch_size, -1), self.hidden)\n","        word_space = self.linear(lstm_out.view(-1, self.embedding_dim))\n","        output = F.log_softmax(word_space, dim=1)\n","        return output\n","\n","\n","def predict_save(model, inputs, path):\n","    #load trained model\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = LSTMLanguageModel(len(vocabulary))\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","    loss_function = nn.NLLLoss()\n","    if torch.cuda.is_available():\n","        model.cuda()\n","    \n","    \n","    #predict with inputs, get logp(w_i,i)\n","    log_ws = []\n","    for sentence in inputs:\n","        sentence_in = torch.tensor(prepare_sequence(sentence, vocabulary), device = device)\n","        logps = model1(sentence_in[:-1])\n","        logpw = []\n","        for i, logp in enumerate(logps):\n","            logpw.append(str(logp[sentence_in[i+1]].item()))\n","        log_ws.append(logpw)\n","    \n","    #write the results to a file\n","    f = open(path,'w')\n","    for log_w in log_ws:\n","        s = str(\" \").join(log_w)\n","        f.write(s+'\\n')\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sSMVNtTVVJF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"deb28b86-816d-4876-cd85-8715efcb07fc","executionInfo":{"status":"ok","timestamp":1575586766928,"user_tz":300,"elapsed":860,"user":{"displayName":"Yining Liu","photoUrl":"","userId":"03197070414203380537"}}},"source":["# main\n","\n","# load data\n","trn_tokens, dev_tokens, max_length = load_data(\"/content/drive/My Drive/Colab Notebooks/NLP/Word-Embeddings-and-Language-Modeling\"+\"/data-for-lang-modeling/\")\n","vocabulary = buildVocabulary(trn_tokens)\n","\n","\n","# train model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = LSTMLanguageModel(len(vocabulary))\n","# loss_function = nn.NLLLoss()\n","loss_function = nn.CrossEntropyLoss().cuda()\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","if torch.cuda.is_available():\n","  model.cuda()\n","    \n","i = 1\n","epoch_index = 1\n","best_loss = 100\n","\n","losses = []\n","trn = trn_tokens\n","for epoch in range(EPOCH_SIZE):\n","    avg_loss = 0.0\n","    for sentence in trn:\n","        # clear out gradients\n","        model.zero_grad()\n","\n","        model.hidden = model.init_hidden()\n","        \n","        # get inputs \n","        sentence_in = torch.tensor(prepare_sequence(sentence, vocabulary)[:-1], device = device)\n","        targets = torch.tensor(prepare_sequence(sentence, vocabulary)[1:], device = device)\n","        \n","        # run for word scores\n","        word_scores = model(sentence_in)\n","        \n","        # compute loss, gradients, update parameters\n","        loss = loss_function(word_scores, targets)\n","        avg_loss += loss.item()\n","        if i%len(trn)==0: \n","          print(\"Epoch:\" ,epoch_index , \"Loss: \",avg_loss/len(trn))\n","          i -= len(trn)\n","          epoch_index += 1\n","        loss.backward()\n","        optimizer.step()\n","        i += 1\n","    avg_loss /= len(trn)\n","    if avg_loss < best_loss:\n","        torch.save(model.state_dict(), MODEL_PATH)\n","        best_loss = avg_loss\n","\n","print(\"Best Loss is: \", best_loss)\n","\n","model1 = LSTMLanguageModel(len(vocabulary))\n","model1.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","loss_function = nn.NLLLoss()\n","if torch.cuda.is_available():\n","  model1.cuda()\n","\n","predict_save(model1, trn_tokens, TRN_RESULT_PATH)\n","predict_save(model1, dev_tokens, DEV_RESULT_PATH)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Training data ...\n","17556\n","Development data ...\n","1841\n","-11.438905715942383 -7.196056365966797 -11.20717716217041 -2.958524703979492 -3.0025978088378906 -11.661580085754395 -9.010720252990723 -12.267210960388184 -11.84308910369873 -11.341996192932129 -3.8602547645568848 -1.7423725128173828 -10.549013137817383 -6.157644748687744 -9.778953552246094 -8.93882942199707 -3.342378616333008 -6.3037567138671875 -11.581682205200195 -11.68324089050293 -9.014998435974121 -8.828913688659668 -8.552507400512695 -4.238045692443848 -2.84897518157959 -10.614652633666992 -7.629580497741699 -4.154504299163818 -8.855761528015137 -8.388718605041504 -7.03987979888916 -8.256608963012695 -4.0765862464904785 -11.946480751037598 -3.009489059448242 -12.046753883361816 -4.990108966827393 -1.7383575439453125 -9.599706649780273 -11.734402656555176 -7.622889041900635 -2.936783790588379 -6.940058708190918 -1.7941474914550781 -3.1224565505981445 -7.620771884918213 -4.630189418792725 -3.2514476776123047 -3.0342493057250977 -6.5934624671936035 -7.10150671005249 -3.4585742950439453 -1.4260549545288086 -10.914220809936523 -6.423066139221191 -4.171926975250244 -2.948232650756836 -5.3180646896362305 -10.728926658630371 -3.7740378379821777 -11.386570930480957 -3.1093807220458984 -8.761163711547852 -4.793477535247803 -6.7096333503723145 -10.537117004394531 -4.392522811889648 -5.514092922210693 -11.485560417175293 -3.589120864868164 -7.020873069763184 -9.67385482788086 -10.461584091186523 -2.730557441711426 -2.0511035919189453 -4.465709686279297 -6.62266206741333 -3.5175113677978516 -8.720987319946289 -3.575791358947754 -11.177659034729004 -4.8245463371276855 -11.593567848205566 -8.074604988098145 -8.982322692871094 -9.654629707336426 -3.370272636413574 -8.528061866760254 -2.1849212646484375 -12.195748329162598 -6.643443584442139 -1.100346565246582 -5.523624897003174 -11.401885986328125 -7.496467113494873 -6.139667987823486 -9.934189796447754 -9.927006721496582 -9.08161735534668 -8.344167709350586 -3.180896759033203 -5.272567272186279 -11.890877723693848 -7.611123085021973 -3.1565351486206055 -9.547164916992188 -8.899347305297852 -3.281853675842285 -12.11312198638916 -4.429919242858887 2125\n","-2.53810977935791 -5.5235595703125 -7.491466522216797 -8.241527557373047 -2.569154739379883 -2.9413204193115234 -10.929247856140137 -6.759863376617432 -4.582960605621338 -5.593412399291992 -9.453897476196289 -1.8178796768188477 -1.6203546524047852 -6.504338264465332 -9.225465774536133 -4.566464424133301 -11.795257568359375 -11.803570747375488 -8.035797119140625 -6.626755237579346 -4.5464911460876465 -10.49134635925293 -2.9877138137817383 -8.580181121826172 -8.242303848266602 -2.994657516479492 -1.4903783798217773 -5.5102620124816895 -5.3830885887146 -6.033823490142822 -11.938484191894531 -9.937541961669922 -12.304046630859375 -6.055497169494629 -3.6170730590820312 -8.511612892150879 -3.26153564453125 -5.508041858673096 -6.877757549285889 -3.9530744552612305 -4.612451553344727 -8.091862678527832 -12.347647666931152 -8.507209777832031 -11.140254974365234 -3.6772146224975586 -12.94389533996582 -3.038055419921875 -8.960376739501953 -12.420154571533203 -12.712353706359863 -6.796304702758789 -8.535506248474121 -4.467594146728516 -9.44853401184082 -12.509238243103027 -7.447580814361572 -4.879628658294678 -11.742288589477539 -11.529669761657715 -7.937914848327637 -9.089462280273438 -12.660534858703613 -12.183252334594727 -4.310220718383789 -5.704239845275879 -7.658799171447754 -2.672366142272949 -9.69925308227539 -10.839269638061523 -3.5580997467041016 -8.256139755249023 -4.303534507751465 -5.576971530914307 -4.153916358947754 -8.429195404052734 -9.375406265258789 -4.234987735748291 -10.180761337280273 -4.321738243103027 -6.0645060539245605 -12.225001335144043 -4.030331611633301 1597\n"],"name":"stdout"}]}]}