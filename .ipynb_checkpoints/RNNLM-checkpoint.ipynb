{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# input_dim = 5\n",
    "# hidden_dim = 10\n",
    "# n_layers = 1\n",
    "\n",
    "# batch_size = 1\n",
    "# seq_len = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ...\n",
      "17556\n",
      "Development data ...\n",
      "1841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27767"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_tokens, dev_tokens, max_length = load_data(os.getcwd()+\"/data-for-lang-modeling/\")\n",
    "vocabulary = buildVocabulary(trn_tokens)\n",
    "trn_x, dev_x = index_tokens(trn_tokens, vocabulary), index_tokens(dev_tokens, vocabulary)\n",
    "# padding(trn_x, max_length)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 12,\n",
       " 28,\n",
       " 77,\n",
       " 78,\n",
       " 38,\n",
       " 5,\n",
       " 79,\n",
       " 80,\n",
       " 22,\n",
       " 81,\n",
       " 82,\n",
       " 11,\n",
       " 12,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 4,\n",
       " 7,\n",
       " 86,\n",
       " 87,\n",
       " 40,\n",
       " 88,\n",
       " 12,\n",
       " 89,\n",
       " 90,\n",
       " 11,\n",
       " 12,\n",
       " 42,\n",
       " 40,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 17,\n",
       " 96,\n",
       " 12,\n",
       " 28,\n",
       " 97,\n",
       " 6,\n",
       " 34,\n",
       " 42,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 6,\n",
       " 101,\n",
       " 32,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 4,\n",
       " 7,\n",
       " 86,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 22,\n",
       " 81,\n",
       " 115,\n",
       " 11,\n",
       " 116,\n",
       " 117,\n",
       " 12,\n",
       " 118,\n",
       " 12,\n",
       " 28,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 30,\n",
       " 124,\n",
       " 125,\n",
       " 76]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMLanguageModel(len(vocabulary))\n",
    "# print(model)\n",
    "# res = model(torch.tensor(trn_x[1]))\n",
    "# res[0].max()\n",
    "\n",
    "# prepare_sequence(trn_tokens[1], vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111, 27767])\n",
      "10.237525939941406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-a331d8e77f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = LSTMLanguageModel(len(vocabulary))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(trn_x[0])\n",
    "    word_scores = model(inputs)\n",
    "    print(word_scores.size())\n",
    "    \n",
    "i = 0\n",
    "    \n",
    "for epoch in range(3):\n",
    "    for sentence in trn_x[:1000]:\n",
    "        # clear out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # get inputs \n",
    "        sentence_in = torch.tensor(sentence)\n",
    "        targets = torch.tensor(sentence)\n",
    "        \n",
    "        # run for word scores\n",
    "        word_scores = model(sentence_in)\n",
    "        \n",
    "        # compute loss, gradients, update parameters\n",
    "        loss = loss_function(word_scores, targets)\n",
    "        if i%500==0: print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     inputs = torch.tensor(trn_x[0])\n",
    "#     word_scores = model(inputs)\n",
    "#     print(word_scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Load data\n",
    "    trn_texts = open(path+\"trn-wiki.txt\").read().strip().split(\"\\n\")\n",
    "    dev_texts = open(path+\"dev-wiki.txt\").read().strip().split(\"\\n\")\n",
    "    print(\"Training data ...\")\n",
    "    print(\"%d\" % (len(trn_texts)))\n",
    "    print(\"Development data ...\")\n",
    "    print(\"%d\" % (len(dev_texts)))\n",
    "\n",
    "    #tokenize data\n",
    "    trn_tokens = [trn_text.split(\" \") for trn_text in trn_texts]\n",
    "    dev_tokens = [dev_text.split(\" \") for dev_text in dev_texts]\n",
    "    \n",
    "    #get max feature size\n",
    "    max_length = max(max([len(tokens) for tokens in trn_tokens]), max([len(tokens) for tokens in dev_tokens]))\n",
    "    \n",
    "    return trn_tokens, dev_tokens, max_length\n",
    "\n",
    "\n",
    "def buildVocabulary(trn_tokens):\n",
    "    words = set()\n",
    "    vocab = {}\n",
    "    index = 1\n",
    "    for tokens in trn_tokens:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def index_tokens(tokens, vocabulary):\n",
    "    tokens_index_list = []\n",
    "    for tokens in tokens_list:\n",
    "        tokens_index = [vocabulary[token] for token in tokens]\n",
    "        tokens_index_list.append(tokens_index)\n",
    "    return tokens_index_list    \n",
    "\n",
    "def prepare_sequence(tokens, vocabulary):\n",
    "#     tokens_index_list = []\n",
    "#     for tokens in tokens_list:\n",
    "#         tokens_index = [vocabulary[token] for token in tokens]\n",
    "#         tokens_index_list.append(tokens_index)\n",
    "    tokens_index = [vocabulary[token] for token in tokens]\n",
    "    return tokens_index    \n",
    "\n",
    "\n",
    "# def padding(tokens_index_list, seq_len):\n",
    "#     features = np.zeros((len(tokens_index_list), seq_len),dtype=int)\n",
    "#     for i, tokens_index in enumerate(tokens_index_list):\n",
    "#         if len(tokens_index) != 0:\n",
    "#             features[i, -len(tokens_index):] = np.array(tokens_index)[:seq_len]\n",
    "#     return features\n",
    "\n",
    "\n",
    "# class LSTMLanguageModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim = 32, batch_size = 1, hidden_dim=32, num_layers=1):\n",
    "#         super(LSTMLanguageModel, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_layers = num_layers\n",
    "#         self.embedding_dim = embedding_dim\n",
    "        \n",
    "#         # Initialize a default nn.Embedding as word embeddings\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         # LSTM layer\n",
    "#         self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = num_layers)\n",
    "        \n",
    "#         # Linear output layer\n",
    "#         self.linear_output = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         init_range = 0.1\n",
    "#         self.word_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "#         self.linear_output.bias.data.fill_(0.0)\n",
    "#         self.linear_output.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         return (Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()),\n",
    "#                 Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()))\n",
    "        \n",
    "#     def forward(self, inputs, hidden):\n",
    "#         embeds = self.word_embeddings(inputs)\n",
    "#         lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "#         # flatted output\n",
    "#         output = self.linear_output(lstm_out.view(-1, self.embedding_dim))\n",
    "#         return output, hidden\n",
    "\n",
    "    \n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim = 32, batch_size = 1, hidden_dim=32, num_layers=1):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize a default nn.Embedding as word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = num_layers)\n",
    "        \n",
    "        # Linear mid layer to transfer hidden to probablities\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         init_range = 0.1\n",
    "#         self.word_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "#         self.linear_output.bias.data.fill_(0.0)\n",
    "#         self.linear_output.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         return (Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()),\n",
    "#                 Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, hidden = self.lstm(embeds.view(len(sentence), self.batch_size, -1))\n",
    "        word_space = self.linear(lstm_out.view(-1, self.embedding_dim))\n",
    "        output = F.log_softmax(word_space, dim=1)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
