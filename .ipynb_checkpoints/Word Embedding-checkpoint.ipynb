{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ...\n",
      "40000, 40000\n",
      "Development data ...\n",
      "5000, 5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-4b8c40e634e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# build sentence representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrn_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_sentence_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdev_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_sentence_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-6105647aaaae>\u001b[0m in \u001b[0;36mconstruct_sentence_rep\u001b[0;34m(data, word_embedding)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mcurrent_res\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mcurrent_res\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "trn_texts = open(os.getcwd()+\"/data-for-text-classification/trn-reviews.txt\").read().strip().split(\"\\n\")\n",
    "trn_labels = open(os.getcwd()+\"/data-for-text-classification/trn-labels.txt\").read().strip().split(\"\\n\")\n",
    "print(\"Training data ...\")\n",
    "print(\"%d, %d\" % (len(trn_texts), len(trn_labels)))\n",
    "\n",
    "dev_texts = open(os.getcwd()+\"/data-for-text-classification/dev-reviews.txt\").read().strip().split(\"\\n\")\n",
    "dev_labels = open(os.getcwd()+\"/data-for-text-classification/dev-labels.txt\").read().strip().split(\"\\n\")\n",
    "print(\"Development data ...\")\n",
    "print(\"%d, %d\" % (len(dev_texts), len(dev_labels)))\n",
    "\n",
    "\n",
    "# lower and tokenize data\n",
    "trn_tokens = [WordPunctTokenizer().tokenize(trn_text.lower()) for trn_text in trn_texts]\n",
    "dev_tokens = [WordPunctTokenizer().tokenize(dev_text.lower()) for dev_text in dev_texts]\n",
    "\n",
    "\n",
    "# load pre-trained 50D word embedding\n",
    "word_embedding = load_word_embedding('glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "\n",
    "# build sentence representations\n",
    "trn_representations = construct_sentence_rep(trn_tokens, word_embedding)\n",
    "dev_representations = construct_sentence_rep(dev_tokens, word_embedding)\n",
    "\n",
    "\n",
    "# Logistic Regression Text Classification using sentence representations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logestic_regression(trn_representations, trn_labels, dev_representations, dev_labels)\n",
    "\n",
    "\n",
    "# use CountVectorizer to vectorize the text\n",
    "vectorizer = CountVectorizer()\n",
    "trn_x = vectorizer.fit_transform(trn_texts).toarray()\n",
    "dev_x = vectorizer.transform(dev_texts).toarray()\n",
    "\n",
    "# concatenate x with sentence representations\n",
    "combine_trn_x = np.concatenate((trn_representations,trn_x), axis=1)\n",
    "combine_dev_x = np.concatenate((dev_representations,dev_x), axis=1)\n",
    "\n",
    "\n",
    "# Logistic Regression Text Classification using features combined sentence representations and vectorizer\n",
    "logestic_regression(combine_trn_x, trn_labels, combine_dev_x, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_dev_x = np.concatenate((dev_representations,dev_x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embedding(Path):\n",
    "    file = open(Path, 'r')\n",
    "    embedding = {}\n",
    "    for line in file:\n",
    "        line = line.split()\n",
    "        embedding[line[0]] = np.array(line[1:], dtype = float)\n",
    "    return embedding\n",
    "\n",
    "def construct_sentence_rep(data, word_embedding):\n",
    "    sentence_reps = []\n",
    "    for tokens in data:\n",
    "        current_res = np.zeros((50,), dtype=float)\n",
    "        for token in tokens:\n",
    "            if token in word_embedding:\n",
    "                current_res += word_embedding[token]\n",
    "            else:\n",
    "                current_res += word_embedding['unk']\n",
    "        if len(tokens)==0: \n",
    "            current_res = word_embedding['unk']\n",
    "        else:\n",
    "            current_res /= len(tokens)\n",
    "        sentence_reps.append(current_res)\n",
    "    return np.array(sentence_reps, dtype=float)\n",
    "\n",
    "def logestic_regression(trn_x, trn_labels, dev_x, dev_labels):\n",
    "    # Define a LR classifier\n",
    "    # classifier = LogisticRegression(random_state=0,tol=0.0002,solver='liblinear', multi_class='auto')\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(trn_x, trn_labels)\n",
    "\n",
    "    # Measure the performance on dev data\n",
    "    # print(\"Training accuracy = %f\" % classifier.score(trn_x, trn_labels))\n",
    "    print(\"Dev accuracy = \", classifier.score(dev_x, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
